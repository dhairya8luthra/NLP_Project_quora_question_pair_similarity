{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0ea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle QQP loaded: 404287 samples\n",
      "Hugging Face GLUE QQP loaded: 40430 samples\n",
      "Model loaded: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Load Kaggle dataset ---\n",
    "df_kaggle = pd.read_csv(\"train.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "train_df_kaggle, val_df_kaggle = train_test_split(df_kaggle, test_size=0.2, random_state=42)\n",
    "y_val_kaggle = val_df_kaggle[\"is_duplicate\"].values\n",
    "print(\"Kaggle QQP loaded:\", len(df_kaggle), \"samples\")\n",
    "\n",
    "# --- Load Hugging Face GLUE QQP dataset ---\n",
    "dataset = load_dataset(\"glue\", \"qqp\")\n",
    "df_hf = dataset[\"validation\"].to_pandas().dropna(subset=[\"question1\", \"question2\"])\n",
    "y_val_hf = df_hf[\"label\"].values\n",
    "print(\"Hugging Face GLUE QQP loaded:\", len(df_hf), \"samples\")\n",
    "\n",
    "# --- Load SBERT model ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded: all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cf9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Kaggle QQP validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2527/2527 [03:09<00:00, 13.36it/s]\n",
      "Batches: 100%|██████████| 2527/2527 [03:10<00:00, 13.25it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding Kaggle QQP validation set...\")\n",
    "emb1_kaggle = model.encode(val_df_kaggle[\"question1\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "emb2_kaggle = model.encode(val_df_kaggle[\"question2\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864561b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing cosine similarities: 100%|██████████| 158/158 [00:00<00:00, 749.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Kaggle QQP Results ===\n",
      "Accuracy: 0.7713646145093869\n",
      "F1: 0.7385037554634567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Function to compute cosine similarities in smaller batches\n",
    "def batched_cosine_sim(emb1, emb2, batch_size=512):\n",
    "    sims = []\n",
    "    for i in tqdm(range(0, len(emb1), batch_size), desc=\"Computing cosine similarities\"):\n",
    "        batch_emb1 = emb1[i:i+batch_size]\n",
    "        batch_emb2 = emb2[i:i+batch_size]\n",
    "        batch_sims = util.pytorch_cos_sim(batch_emb1, batch_emb2).diagonal()\n",
    "        sims.append(batch_sims.cpu())\n",
    "    return torch.cat(sims).numpy()\n",
    "\n",
    "# --- Kaggle evaluation ---\n",
    "cosine_scores_kaggle = batched_cosine_sim(emb1_kaggle, emb2_kaggle, batch_size=512)\n",
    "threshold = 0.75  # can tune later\n",
    "\n",
    "preds_kaggle = (cosine_scores_kaggle > threshold).astype(int)\n",
    "print(\"\\n=== Kaggle QQP Results ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_kaggle, preds_kaggle))\n",
    "print(\"F1:\", f1_score(y_val_kaggle, preds_kaggle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding Hugging Face GLUE QQP validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1264/1264 [01:35<00:00, 13.18it/s]\n",
      "Batches: 100%|██████████| 1264/1264 [01:37<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hugging Face GLUE QQP Results ===\n",
      "Accuracy: 0.7708632203809053\n",
      "F1: 0.736982567713361\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEncoding Hugging Face GLUE QQP validation set...\")\n",
    "emb1_hf = model.encode(df_hf[\"question1\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "emb2_hf = model.encode(df_hf[\"question2\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Use the same memory-safe cosine function\n",
    "cosine_scores_hf = batched_cosine_sim(emb1_hf, emb2_hf, batch_size=512)\n",
    "\n",
    "preds_hf = (cosine_scores_hf > threshold).astype(int)\n",
    "\n",
    "print(\"\\n=== Hugging Face GLUE QQP Results ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_hf, preds_hf))\n",
    "print(\"F1:\", f1_score(y_val_hf, preds_hf))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
