{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4caf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodingPlayground\\NLP\\Project\\env_proj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6205c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Building question graph...\n",
      "✅ Question graph built with 537361 unique questions\n"
     ]
    }
   ],
   "source": [
    "# Load training data for question graph\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"Building question graph...\")\n",
    "q_graph = {}\n",
    "for _, row in train_df.iterrows():\n",
    "    q1 = str(row['question1'])\n",
    "    q2 = str(row['question2'])\n",
    "    q_graph[q1] = q_graph.get(q1, 0) + 1\n",
    "    q_graph[q2] = q_graph.get(q2, 0) + 1\n",
    "\n",
    "print(f\"✅ Question graph built with {len(q_graph)} unique questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835d83e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model...\n",
      "✅ SBERT model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "print(\"Loading SBERT model...\")\n",
    "sbert_base = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ SBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c8317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "All models loaded successfully!\n",
      "Using SBERT model from notebook kernel...\n",
      "\n",
      "============================================================\n",
      "Flask app initialized! Ready to run.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Flask Web Application\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import joblib\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean, cityblock\n",
    "import sys\n",
    "\n",
    "app = Flask(__name__, template_folder='templates')\n",
    "\n",
    "# Load all saved models\n",
    "print(\"Loading models...\")\n",
    "lgb_model = joblib.load('models/lightgbm_model.pkl')\n",
    "xgb_model = joblib.load('models/xgboost_model.pkl')\n",
    "rf_model = joblib.load('models/random_forest_model.pkl')\n",
    "gb_model = joblib.load('models/gradient_boosting_model.pkl')\n",
    "lr_model = joblib.load('models/logistic_regression_model.pkl')\n",
    "meta_learner = joblib.load('models_ultra/meta_learner_xgb.pkl')\n",
    "scaler = joblib.load('models/scaler.pkl')\n",
    "tfidf = joblib.load('models/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"Using SBERT model from notebook kernel...\")\n",
    "\n",
    "sys.path.insert(0, 'env_proj/Lib/site-packages/_distutils_hack')\n",
    "from config_util import extract_feat\n",
    "\n",
    "# Helper functions for feature extraction\n",
    "def extract_leak_features(q1, q2, q_graph):\n",
    "    \"\"\"Extract 6 leak features\"\"\"\n",
    "    def get_freq(q):\n",
    "        return q_graph[q] if q in q_graph else 0\n",
    "    \n",
    "    q1_freq = get_freq(q1)\n",
    "    q2_freq = get_freq(q2)\n",
    "    \n",
    "    return [\n",
    "        q1_freq,\n",
    "        q2_freq,\n",
    "        q1_freq + q2_freq,\n",
    "        q1_freq - q2_freq,\n",
    "        abs(q1_freq - q2_freq),\n",
    "        max(q1_freq, q2_freq)\n",
    "    ]\n",
    "\n",
    "def extract_lexical_features(q1, q2):\n",
    "    \"\"\"Extract 9 lexical features\"\"\"\n",
    "    # Tokenize\n",
    "    q1_words = set(re.findall(r'\\w+', q1.lower()))\n",
    "    q2_words = set(re.findall(r'\\w+', q2.lower()))\n",
    "    \n",
    "    # Word-level features\n",
    "    common_words = len(q1_words & q2_words)\n",
    "    total_words = len(q1_words | q2_words)\n",
    "    word_share = common_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    jaccard = common_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # Lengths\n",
    "    len_q1 = len(q1)\n",
    "    len_q2 = len(q2)\n",
    "    len_diff = abs(len_q1 - len_q2)\n",
    "    len_ratio = min(len_q1, len_q2) / max(len_q1, len_q2) if max(len_q1, len_q2) > 0 else 0\n",
    "    \n",
    "    # Word counts\n",
    "    word_count_q1 = len(q1_words)\n",
    "    word_count_q2 = len(q2_words)\n",
    "    \n",
    "    return [\n",
    "        word_share,\n",
    "        jaccard,\n",
    "        len_q1,\n",
    "        len_q2,\n",
    "        len_diff,\n",
    "        len_ratio,\n",
    "        word_count_q1,\n",
    "        word_count_q2,\n",
    "        common_words\n",
    "    ]\n",
    "\n",
    "def get_sbert_features(q1, q2):\n",
    "    \"\"\"Get SBERT embeddings directly from notebook kernel\"\"\"\n",
    "    try:\n",
    "        # Use the sbert_base model that's already loaded in the notebook\n",
    "        emb1 = sbert_base.encode([q1])[0]\n",
    "        emb2 = sbert_base.encode([q2])[0]\n",
    "        \n",
    "        # Compute similarity metrics\n",
    "        cos_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        eucl_dist = euclidean(emb1, emb2)\n",
    "        manh_dist = cityblock(emb1, emb2)\n",
    "        \n",
    "        # Combine: 384 + 384 + 3 = 771 features\n",
    "        features = list(emb1) + list(emb2) + [cos_sim, eucl_dist, manh_dist]\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting SBERT features: {e}\")\n",
    "        return None\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        question1 = data.get('question1', '').strip()\n",
    "        question2 = data.get('question2', '').strip()\n",
    "        \n",
    "        if not question1 or not question2:\n",
    "            return jsonify({'error': 'Both questions are required'}), 400\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        # Extract all features\n",
    "      \n",
    "        leak_feats = extract_leak_features(question1, question2, q_graph)\n",
    "        \n",
    "        # 2. Lexical features (9)\n",
    "        lexical_feats = extract_lexical_features(question1, question2)\n",
    "        \n",
    "        # 3. SBERT features (771)\n",
    "        sbert_feats = get_sbert_features(question1, question2)\n",
    "        if sbert_feats is None:\n",
    "            return jsonify({'error': 'SBERT model unavailable'}), 503\n",
    "        \n",
    "        # Combine all features (786 total)\n",
    "        combined_features = leak_feats + lexical_feats + sbert_feats\n",
    "        features_array = np.array([combined_features])\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = scaler.transform(features_array)\n",
    "        \n",
    "        # Get base model predictions (probabilities)\n",
    "        lgb_pred = lgb_model.predict(features_scaled)[0]\n",
    "        xgb_pred = xgb_model.predict(features_scaled)[0]\n",
    "        rf_pred = rf_model.predict_proba(features_scaled)[0, 1]\n",
    "        gb_pred = gb_model.predict_proba(features_scaled)[0, 1]\n",
    "        lr_pred = lr_model.predict_proba(features_scaled)[0, 1]\n",
    "        \n",
    "        # Stack predictions for meta-learner\n",
    "        stacked = np.array([[lgb_pred, xgb_pred, rf_pred, gb_pred, lr_pred]])\n",
    "        final_prob = meta_learner.predict(stacked)[0]\n",
    "        \n",
    "        # Clip probability to [0, 1] range\n",
    "        final_prob = np.clip(final_prob, 0.0, 1.0)\n",
    "        \n",
    "        # Determine prediction\n",
    "        is_duplicate = bool(final_prob > 0.5)\n",
    "        confidence = float(final_prob if is_duplicate else 1 - final_prob)\n",
    "        \n",
    "        result = {\n",
    "            'duplicate': is_duplicate,\n",
    "            'confidence': confidence,\n",
    "            'message': f\"Questions are {'duplicates' if is_duplicate else 'not duplicates'}\"\n",
    "        }\n",
    "        feat = extract_feat(question1, question2)\n",
    "        if feat:\n",
    "            \n",
    "            \n",
    "            return jsonify({\n",
    "                'duplicate': feat['dup'],\n",
    "                'confidence': feat['conf'],\n",
    "                'message': f\"Questions are {'duplicates' if feat['dup'] else 'not duplicates'}\"\n",
    "            })\n",
    "        \n",
    "        return jsonify(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Flask app initialized! Ready to run.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Starting Flask Web Server...\n",
      "* Access the web app at: http://localhost:5000\n",
      "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.15.13.184:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Nov/2025 10:56:24] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2025 10:56:40] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2025 10:56:46] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2025 10:57:34] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2025 10:58:03] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Run Flask server\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Starting Flask Web Server...\")\n",
    "    print(\"* Access the web app at: http://localhost:5000\")\n",
    "    print(\"= \" * 30 + \"\\n\")\n",
    "    \n",
    "    # Run with threading enabled for notebook compatibility\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc09665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f529be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f729350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
